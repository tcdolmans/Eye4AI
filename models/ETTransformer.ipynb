{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (self.head_dim * heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "    \n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        \n",
    "        # Split embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
    "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
    "        queries = self.queries(queries)  # (N, query_len, heads, head_dim)\n",
    "        \n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])  # Good source on Einsum: https://www.youtube.com/watch?v=pkVwUVEHmfI\n",
    "        # Queries shape: (N, query_len, heads, head_dim)\n",
    "        # Keys shape: (N, key_len, heads, head_dim)\n",
    "        # Energy shape: (N, heads, query_len, key_len)\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        \n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim)\n",
    "        # Attention shape: (N, heads, query_len, key_len)\n",
    "        # Values shape: (N, value_len, heads, head_dim)\\\n",
    "        # Out shape : (N, query_len, heads, head_dim), then flatten last 2 dims\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,  # NOTE: Continuous rather than fixed vocab size\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)  # NOTE: Retrieve various embeddings from embedder classes\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))  # NOTE: Convert to multimodal embedding\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_size,\n",
    "                 heads,\n",
    "                 forward_expansion,\n",
    "                 dropout,\n",
    "                 device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 trg_vocab_size,  # NOTE: Continuous rather than fixed vocab size\n",
    "                 embed_size,\n",
    "                 num_layers,\n",
    "                 heads,\n",
    "                 forward_expansion,\n",
    "                 dropout,\n",
    "                 device,\n",
    "                 max_length):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)  # NOTE: Convert to multimodal embedding\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    forward_expansion,\n",
    "                    dropout,\n",
    "                    device\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)  # NOTE: Convert to multimodal embedding\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "        out = self.fc_out(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_vocab_size,  # NOTE: Continuous rather than fixed vocab size\n",
    "                 trg_vocab_size,  # NOTE: Continuous rather than fixed vocab size\n",
    "                 src_pad_idx,\n",
    "                 trg_pad_idx,\n",
    "                 embed_size=256,\n",
    "                 num_layers=6,\n",
    "                 forward_expansion=4,\n",
    "                 heads=8,\n",
    "                 dropout=0,\n",
    "                 device=\"cuda\",\n",
    "                 max_length=300):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,  # NOTE: Continuous rather than fixed vocab size\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,   # NOTE: Continuous rather than fixed vocab size\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "    \n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # src_mask shape: (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "        # trg_mask shape: (N, 1, trg_len, trg_len)\n",
    "        return trg_mask.to(self.device)\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [768, 300, 15], expected input[8, 3, 300] to have 300 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 165\u001b[0m\n\u001b[0;32m    162\u001b[0m sem_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m8\u001b[39m, \u001b[39m12\u001b[39m, \u001b[39m600\u001b[39m, \u001b[39m800\u001b[39m)\n\u001b[0;32m    164\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m output \u001b[39m=\u001b[39m model(et_data, img_data, sem_data)\n\u001b[0;32m    166\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\Tenzing Dolmans\\miniconda3\\envs\\thesis1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[66], line 136\u001b[0m, in \u001b[0;36mMultimodalBottleneckTransformer.forward\u001b[1;34m(self, et_data, img_data, sem_data)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, et_data, img_data, sem_data):\n\u001b[1;32m--> 136\u001b[0m     et_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49met_embed(et_data)\n\u001b[0;32m    137\u001b[0m     img_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_embed(img_data)\n\u001b[0;32m    138\u001b[0m     sem_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msem_embed(sem_data)\n",
      "File \u001b[1;32mc:\\Users\\Tenzing Dolmans\\miniconda3\\envs\\thesis1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[66], line 42\u001b[0m, in \u001b[0;36mETPatchEmbed.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     35\u001b[0m     \u001b[39m# if self.OSIE:\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[39m#     x, y = self.oracle.generate_batch(x)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[39m#     return x, y\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[39m# else:\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m---> 42\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprojection(x\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Tenzing Dolmans\\miniconda3\\envs\\thesis1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Tenzing Dolmans\\miniconda3\\envs\\thesis1\\lib\\site-packages\\torch\\nn\\modules\\conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\Tenzing Dolmans\\miniconda3\\envs\\thesis1\\lib\\site-packages\\torch\\nn\\modules\\conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    307\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    308\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    310\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [768, 300, 15], expected input[8, 3, 300] to have 300 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "class ETPatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes a 1D convolution of a patch of ET data to embed and returns the embedded data.\n",
    "    The out.permute() is done to make the data compatible with the 1D convolution over batches.\n",
    "    If OSIE is True, the oracle is used to generate a source and target sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 sample_height=300,\n",
    "                 in_channels=3,\n",
    "                 patch_size=15,\n",
    "                 embed_dim=768,\n",
    "                 kernel_size=15,\n",
    "                 stride=15,\n",
    "                 oracle=None,\n",
    "                 OSIE=False):\n",
    "        super().__init__()\n",
    "        # These three lines are to make the patch embedding compatible with ETTransformer\n",
    "        # -----\n",
    "        sample_size = tuple((sample_height, in_channels))\n",
    "        patch_size = tuple((patch_size, in_channels))\n",
    "        num_patches = (sample_size[1] // patch_size[1] * sample_size[0] // patch_size[0])\n",
    "        # -----\n",
    "        self.sample_size = sample_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.norm = nn.BatchNorm1d(sample_height)\n",
    "        self.projection = nn.Conv1d(in_channels=sample_height,\n",
    "                                    out_channels=embed_dim,\n",
    "                                    kernel_size=kernel_size,\n",
    "                                    stride=stride)\n",
    "        # self.OSIE = OSIE\n",
    "        # self.oracle = oracle\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if self.OSIE:\n",
    "        #     x, y = self.oracle.generate_batch(x)\n",
    "        #     x = self.norm(x)\n",
    "        #     x = self.projection(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        #     return x, y\n",
    "        # else:\n",
    "        x = self.norm(x.permute(0, 2, 1))\n",
    "        x = self.projection(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ImagePatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes a 2D convolution of a patch of an image data to embed in a lower dimensional space.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 img_width=800,\n",
    "                 img_height=600,\n",
    "                 patch_size=25,\n",
    "                 in_channels=3,\n",
    "                 embed_dim=768):\n",
    "        super().__init__()\n",
    "        # self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        # self.num_patches = (self.img_size // self.patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(in_channels=in_channels,\n",
    "                                    out_channels=embed_dim,\n",
    "                                    kernel_size=patch_size,\n",
    "                                    stride=patch_size)\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SemanticEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes semantic tags per pixel of an image and embeds the information in the same space as ImagePatchEmbed.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 img_width=800,\n",
    "                 img_height=600,\n",
    "                 patch_size=25,\n",
    "                 in_channels=12,\n",
    "                 embed_dim=768):\n",
    "        super().__init__()\n",
    "        # self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        # self.num_patches = (self.img_size // self.patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(in_channels=in_channels,\n",
    "                                    out_channels=embed_dim,\n",
    "                                    kernel_size=patch_size,\n",
    "                                    stride=patch_size)\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultimodalBottleneckTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 num_heads,\n",
    "                 et_embed_dim,\n",
    "                 img_embed_dim,\n",
    "                 sem_embed_dim,\n",
    "                 et_patch_size,\n",
    "                 img_patch_size,\n",
    "                 sem_patch_size,\n",
    "                 et_sample_height,\n",
    "                 img_height,\n",
    "                 img_width):\n",
    "        super(MultimodalBottleneckTransformer, self).__init__()\n",
    "        self.et_embed = ETPatchEmbed(sample_height=et_sample_height,\n",
    "                                      in_channels=3,\n",
    "                                      patch_size=et_patch_size,\n",
    "                                      embed_dim=et_embed_dim,\n",
    "                                      kernel_size=et_patch_size,\n",
    "                                      stride=et_patch_size)\n",
    "        self.img_embed = ImagePatchEmbed(img_width=img_width,\n",
    "                                         img_height=img_height,\n",
    "                                         patch_size=img_patch_size,\n",
    "                                         in_channels=3,\n",
    "                                         embed_dim=img_embed_dim)\n",
    "        self.sem_embed = SemanticEmbedding(img_width=img_width,\n",
    "                                           img_height=img_height,\n",
    "                                           patch_size=sem_patch_size,\n",
    "                                           in_channels=12,\n",
    "                                           embed_dim=sem_embed_dim)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=et_embed_dim + img_embed_dim + sem_embed_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=4 * (et_embed_dim + img_embed_dim + sem_embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, et_data, img_data, sem_data):\n",
    "        et_emb = self.et_embed(et_data)\n",
    "        img_emb = self.img_embed(img_data)\n",
    "        sem_emb = self.sem_embed(sem_data)\n",
    "\n",
    "        embeddings = torch.cat([et_emb, img_emb, sem_emb], dim=-1)\n",
    "\n",
    "        transformer_out = self.transformer(embeddings)\n",
    "        return transformer_out\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = MultimodalBottleneckTransformer(num_layers=6,\n",
    "                                        num_heads=8,\n",
    "                                        et_embed_dim=768,\n",
    "                                        img_embed_dim=768,\n",
    "                                        sem_embed_dim=768,\n",
    "                                        et_patch_size=15,\n",
    "                                        img_patch_size=25,\n",
    "                                        sem_patch_size=25,\n",
    "                                        et_sample_height=300,\n",
    "                                        img_height=600,\n",
    "                                        img_width=800)\n",
    "\n",
    "# Initialize input data\n",
    "et_data = torch.randn(8, 3, 300)\n",
    "img_data = torch.randn(8, 3, 600, 800)\n",
    "sem_data = torch.randn(8, 12, 600, 800)\n",
    "\n",
    "# Forward pass\n",
    "output = model(et_data, img_data, sem_data)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
